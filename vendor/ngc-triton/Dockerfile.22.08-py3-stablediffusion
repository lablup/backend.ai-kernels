FROM nvcr.io/nvidia/tritonserver:22.08-py3
# NVIDIA Triton

ENV PYTHONUNBUFFERED=1 \
    _CUDA_COMPAT_PATH="/usr/local/cuda/compat" \
    PATH="/opt/tritonserver/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin" \
    LD_LIBRARY_PATH="/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64" \
    LANG=C.UTF-8

RUN apt update && \
    apt install -y --no-install-recommends \
      ncurses-term \
      unzip zlib1g-dev htop curl git && \
    ln -sf /usr/share/terminfo/x/xterm-color /usr/share/terminfo/x/xterm-256color

# Copy example models
RUN git clone https://github.com/triton-inference-server/server.git /workspace/triton-server && \
    cd /workspace/triton-server/docs/examples && \
    ./fetch_models.sh && \
    chmod -R +r /workspace/triton-server

RUN python3 -m pip install \
    torch torchvision torchaudio \
    transformers ftfy scipy accelerate \
    diffusers==0.9.0 \
    transformers[onnxruntime]

RUN python3 -m pip install \
    streamlit 'tritonclient[grpc]'
RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 2
COPY ./service-defs /etc/backend.ai/service-defs
COPY ./runner-scripts/bootstrap.sh /opt/container/
RUN sed -i -e 's/),1e3)/),1e4)/g'  /usr/local/lib/python3.8/dist-packages/streamlit/static/static/js/main.*.js
RUN sed -i -e 's/baseUriPartsList,500/baseUriPartsList,10000/g' /usr/local/lib/python3.8/dist-packages/streamlit/static/static/js/main.*.js

LABEL ai.backend.kernelspec="1" \
      ai.backend.envs.corecount="OPENBLAS_NUM_THREADS,OMP_NUM_THREADS,NPROC" \
      ai.backend.features="batch query uid-match user-input" \
      ai.backend.role="INFERENCE" \
      ai.backend.base-distro="ubuntu18.04" \
      ai.backend.accelerators="cuda" \
      ai.backend.resource.min.cpu="1" \
      ai.backend.resource.min.mem="1g" \
      ai.backend.resource.min.cuda.device=1 \
      ai.backend.resource.min.cuda.shares=0 \
      ai.backend.model-path="/models" \
      ai.backend.runtime-type="python" \
      ai.backend.runtime-path="/usr/bin/python" \
      ai.backend.service-ports="triton-server-grpc:preopen:8001,gradio-cat-client:http:9000" \
      ai.backend.endpoint-ports="triton-server-grpc" \
      ai.backend.model-format="triton"
      # ai.backend.service-ports="triton-server-http:preopen:8000,triton-server-grpc:preopen:8001,triton-server-metric:preopen:8002"
      # ai.backend.service-ports="triton-server:preopen:[8000,8001,8002]"

WORKDIR /home/work
# vim: ft=dockerfile
